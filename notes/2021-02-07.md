# 7 February, 2021

Reviewing the math, I realized yesterday that a two-layer linear network is
still linear in its expresivity.... duh.
Bishop 2006 S5.1 has an example perfectly fitting a `y=x*x` using a 
three-layer network, but the range is limited to `[-1,1]` and each layer
has a `tanh()` activation function.

By adding a [ReLU activation function] to the first layer of the network
in [fit_quad.rs](../src/bin/fit_quad.rs), I was able to reduce the error
about 25%, but the fit is still miserable.

The folks at CCRi [have a blog post on learning to model quadratic functions from thier roots](https://ccri.com/can-a-neural-net-learn-the-quadratic-formula/)
and point out that the optimization worked out in PyTorch for them only after

a) using first the input domain `(r1+r2) x (r1*r2)` where the `ri` values
are candidate roots for the function to model, `p(x) = x*x - (r1+r2)*x + r1*r2`,
with the desired output being the roots of the function,
b) then refining loss and evaluation function to order the two network outputs.
