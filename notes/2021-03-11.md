# 11 March, 2021

I'm still evaluating the convolutional neural-network implementation.
The basic implementation seems to be straightforward, but there are
matrix operations that could be used in different contexts, specifically
using either statically-dimensioned or dynamically dimensioned matrices.

Rather than have two sets of roughly identical operations, I looked at some
of the nalgebra operations to see that one can just assume an argument
is of type `Matrix<T, R, C, S>` implemented by either `DMatrix` or `MatrixMN`....
The types for `R` and `C` representing the dimension can be deferred later in
compilation, preserving dimension checks.  The storage allocator `S` type is new
to me.
`S` is usually a placeholder for either `nalgebra::storage::Storage<T, R, C>` or `StorageMut`, 
the latter ensuring indexed assignment.

For functions taking an matrix or vector argument, you can just substitute `MatrixMN` or
`VectorN` with `Matrix` and add generic storage parameters.

For return values, it would seem that keeping statically-sized return types is still the
easiest way to go.

For traits such as `Model`, subsituting `Matrix` for the statically-sized arguments is
more complex.
Rust does not allow generic parameters for trait methods, only for the trait itself.
The justification for this is that the compiler must be able to generate the dispatch
table at compile time.

The work around for extending trait methods is to parameterize the trait, which complicates
the declaration somewhat and it means that an implementation of the trait has to use the
same constraints for every invocation of the trait method, I think.... more to think on.

BTW, I realized that nalgebra uses column-major matrix representation, so some of the `conv2d`
code representing matrices with vectors will be updated to better align with the underlying
representation.
